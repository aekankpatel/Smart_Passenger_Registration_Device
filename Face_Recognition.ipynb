{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import face_recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load pre-trained YOLOv3 model\n",
    "we load the pre-trained YOLOv3 model from its configuration file and weights file. This model will be used to detect objects in the video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load COCO class labels\n",
    "\n",
    "We load the class labels from the COCO dataset that the YOLOv3 model was trained on. This will help us identify the type of objects detected by the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the minimum probability threshold for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_confidence = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize empty lists to keep track of the unique facial encodings and labels for each detected person, and the total number of people detected in the video feed.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for person identification\n",
    "known_encodings = []\n",
    "known_labels = []\n",
    "person_count = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function detect_and_identify_people() to detect and identify people in each frame of the video feed. The function takes in a single argument frame which is a numpy array representing a single video frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for detecting and identifying people in a frame\n",
    "def detect_and_identify_people(frame):\n",
    "    global known_encodings, known_labels, person_count\n",
    "\n",
    "    # Get the frame dimensions and construct a blob from the frame\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Set the input blob for the neural network\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Forward pass through the neural network to detect objects\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    # Initialize empty lists for detected persons\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    # Loop over the outputs and find the detected persons\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            # Check if the detected object is a person and meets the minimum confidence threshold\n",
    "            if classes[class_id] == 'person' and confidence > min_confidence:\n",
    "                # Scale the bounding box coordinates to the original frame size\n",
    "                box = detection[0:4] * np.array([w, h, w, h])\n",
    "                (center_x, center_y, width, height) = box.astype('int')\n",
    "\n",
    "                # Get the top-left corner coordinates of the bounding box\n",
    "                x = int(center_x - (width/2))\n",
    "                y = int(center_y - (height/2))\n",
    "\n",
    "                # Add the bounding box coordinates and confidence score to the lists\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, min_confidence, 0.4)\n",
    "\n",
    "    # Loop over the indices and draw the bounding boxes on the frame\n",
    "    for i in indices.flatten():\n",
    "        # Get the top-left corner coordinates of the bounding box and its width and height\n",
    "        x, y = boxes[i][0], boxes[i][1]\n",
    "        w, h = boxes[i][2], boxes[i][3]\n",
    "\n",
    "        if x < 0:\n",
    "            w = w + x\n",
    "            x = 0\n",
    "        if y < 0:\n",
    "            h = h + y\n",
    "            y = 0\n",
    "        if x+w > frame.shape[1]:\n",
    "            w = frame.shape[1] - x\n",
    "        if y+h > frame.shape[0]:\n",
    "            h = frame.shape[0] - y\n",
    "\n",
    "\n",
    "        # Crop the detected person from the frame\n",
    "        person = frame[y:y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        person_array = np.array(person)\n",
    "\n",
    "        # Generate a unique label for the person based on their appearance\n",
    "        encoding = face_recognition.face_encodings(person_array)\n",
    "\n",
    "        if len(encoding) == 0:\n",
    "            continue \n",
    "        \n",
    "        try:\n",
    "            matches = face_recognition.compare_faces(known_encodings, encoding, tolerance=0.6)\n",
    "        except TypeError:\n",
    "            print(\"matches failed\")\n",
    "            matches=[True]\n",
    "        if True in matches:\n",
    "            continue        \n",
    "        label = 'person{}'.format(person_count)\n",
    "        known_encodings.append(encoding)\n",
    "        known_labels.append(label)\n",
    "\n",
    "        # Save the person's image with their label as filename\n",
    "        filename = '{}.jpg'.format(label)\n",
    "        cv2.imwrite(filename, person)\n",
    "\n",
    "        # Increment the person count\n",
    "        person_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open the camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "matches failed\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Capture a frame from the camera\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"cap failed\")\n",
    "        break\n",
    "\n",
    "    detect_and_identify_people(frame)\n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Exit the loop if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "print(person_count)\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
